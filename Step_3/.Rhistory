foreach(j = seq_along(data_list), .combine = 'c', .packages = "dplyr") %dopar% {
data <- data_list[[j]]
total_playtime <- foreach(k = 1:nrow(data), .combine = c, .packages = "dplyr") %dopar% {
calculate_playtime_appid(
steam_id = data$steamid[k],
app_id = data$appid[k],
app_id_treat = data$appid_trt[k],
treat_flg = data$treat_flg[k],
update_code = data$update_id_trt[k],
date_ini = data$Date_Ini[k],
date_end = data$Date_End[k]
)
}
data <- data %>%
mutate(total_playtime = total_playtime)
# Save each dataframe to the list
combined_results[[length(combined_results) + 1]] <- data
}
# Save the modified list to a new folder with the new file name
new_file_name <- paste0("groups_playtime_", combined_results[[1]]$appid_trt[1], "_", combined_results[[1]]$update_id_trt[1], ".rds")
full_path <- file.path(new_dir, new_file_name)
saveRDS(combined_results, file = full_path)
# Return the file path
return(full_path)
}
library(parallel)
library(doParallel)
# Set the directory where the RDS files are stored
new_dir <- "D:/Thesis/Files/ExpGroups/"
# List all the RDS files in the directory
rds_files <- list.files(path = new_dir, pattern = "*.rds", full.names = TRUE)
# Set up a cluster with multiple workers
cl <- makeCluster(detectCores())
# Register the cluster as the parallel backend for foreach
registerDoParallel(cl)
# Apply the function to each RDS file in parallel
file_paths <- foreach(file = rds_files, .combine = c) %dopar% {
process_rds_file(file, new_dir)
}
process_rds_file <- function(file, new_dir) {
library(foreach)
library(dplyr)
# Read the data from the RDS file
data_list <- readRDS(file)
# Initialize an empty list to store the combined results
combined_results <- list()
# Use foreach to parallelize the loop
foreach(j = seq_along(data_list), .combine = 'c', .packages = "dplyr") %dopar% {
data <- data_list[[j]]
total_playtime <- foreach(k = 1:nrow(data), .combine = c, .packages = "dplyr") %dopar% {
calculate_playtime_appid(
steam_id = data$steamid[k],
app_id = data$appid[k],
app_id_treat = data$appid_trt[k],
treat_flg = data$treat_flg[k],
update_code = data$update_id_trt[k],
date_ini = data$Date_Ini[k],
date_end = data$Date_End[k]
)
}
data <- data %>%
mutate(total_playtime = total_playtime)
# Save each dataframe to the list
combined_results[[length(combined_results) + 1]] <- data
}
# Save the modified list to a new folder with the new file name
new_file_name <- paste0("groups_playtime_", combined_results[[1]]$appid_trt[1], "_", combined_results[[1]]$update_id_trt[1], ".rds")
full_path <- file.path(new_dir, new_file_name)
saveRDS(combined_results, file = full_path)
# Return the file path
return(full_path)
}
library(parallel)
library(doParallel)
# Set the directory where the RDS files are stored
new_dir <- "D:/Thesis/Files/ExpGroups_playtime/"
# List all the RDS files in the directory
rds_files <- list.files(path = new_dir, pattern = "*.rds", full.names = TRUE)
# Set up a cluster with multiple workers
cl <- makeCluster(detectCores())
# Register the cluster as the parallel backend for foreach
registerDoParallel(cl)
# Apply the function to each RDS file in parallel
file_paths <- foreach(file = rds_files, .combine = c) %dopar% {
process_rds_file(file, new_dir)
}
# Set the directory where the RDS files are stored
new_dir <- "D:/Thesis/Files/ExpGroups/"
pt_dir <- "D:/Thesis/Files/ExpGroups_playtime/"
calculate_playtime_appid <- function(steam_id, app_id, app_id_treat, treat_flg, update_code, date_ini, date_end) {
# Build the file path based on the app_id
rds_file_control <- paste0("D:/Thesis/Files/appids_control", "/", app_id_treat, "_", update_code, ".rds")
rds_file_treat <- paste0("D:/Thesis/Files/appids_treatment", "/", app_id_treat, ".rds")
appid_control <- distinct(readRDS(rds_file_control))
appid_trt <- distinct(readRDS(rds_file_treat))
# Filter the data based on the steam_id and app_id
appid_control <- appid_control %>% filter(steamid == steam_id, appid == app_id)
appid_trt <- appid_trt %>% filter(steamid == steam_id, appid == app_id, update_id == update_code)
# Check if the dataframes have zero rows based on treat_flg, and return 0 if true
if (treat_flg == 1 && nrow(appid_trt) == 0) {
return(0)
} else if (treat_flg == 0 && nrow(appid_control) == 0) {
return(0)
}
all_dates <- as.Date(colnames(appid_control)[3:length(colnames(appid_control))], format = "%Y-%m-%d")
date_indices_in_range <- which(all_dates >= date_ini & all_dates <= date_end)
appid_control <- appid_control %>%
filter(rowSums(.[,  date_indices_in_range + 2], na.rm = TRUE) != 0) %>%
select(steamid, appid, (date_indices_in_range + 2))
if (treat_flg == 1) {
playtime_rows <- appid_trt
} else {
playtime_rows <- appid_control
}
# Add a warning if the number of rows in playtime_rows is not equal to 1
if (nrow(playtime_rows) != 1) {
warning("playtime_rows has more than one row. Please check the input data.")
}
if (length(date_indices_in_range) == 0) {
return(0)
} else {
if (treat_flg == 1) {
return(sum(playtime_rows[1, (seq_along(date_indices_in_range) + 3)], na.rm = TRUE))
} else {
return(sum(playtime_rows[1, (seq_along(date_indices_in_range) + 2)], na.rm = TRUE))
}
}
print(paste0("Processing file ", app_id_treat,"-", treat_flg, "."))
}
process_rds_file <- function(file, new_dir) {
library(foreach)
library(dplyr)
# Read the data from the RDS file
data_list <- readRDS(file)
# Initialize an empty list to store the combined results
combined_results <- list()
# Use foreach to parallelize the loop
foreach(j = seq_along(data_list), .combine = 'c', .packages = "dplyr") %dopar% {
data <- data_list[[j]]
total_playtime <- foreach(k = 1:nrow(data), .combine = c, .packages = "dplyr") %dopar% {
calculate_playtime_appid(
steam_id = data$steamid[k],
app_id = data$appid[k],
app_id_treat = data$appid_trt[k],
treat_flg = data$treat_flg[k],
update_code = data$update_id_trt[k],
date_ini = data$Date_Ini[k],
date_end = data$Date_End[k]
)
}
data <- data %>%
mutate(total_playtime = total_playtime)
# Save each dataframe to the list
combined_results[[length(combined_results) + 1]] <- data
}
# Save the modified list to a new folder with the new file name
new_file_name <- paste0("groups_playtime_", combined_results[[1]]$appid_trt[1], "_", combined_results[[1]]$update_id_trt[1], ".rds")
full_path <- file.path(pt_dir, new_file_name)
saveRDS(combined_results, file = full_path)
# Return the file path
return(full_path)
}
library(parallel)
library(doParallel)
# Set the directory where the RDS files are stored
new_dir <- "D:/Thesis/Files/ExpGroups/"
pt_dir <- "D:/Thesis/Files/ExpGroups_playtime/"
# List all the RDS files in the directory
rds_files <- list.files(path = new_dir, pattern = "*.rds", full.names = TRUE)
# Set up a cluster with multiple workers
cl <- makeCluster(detectCores())
# Register the cluster as the parallel backend for foreach
registerDoParallel(cl)
# Apply the function to each RDS file in parallel
file_paths <- foreach(file = rds_files, .combine = c) %dopar% {
process_rds_file(file, new_dir)
}
calculate_playtime_appid <- function(steam_id, app_id, app_id_treat, treat_flg, update_code, date_ini, date_end) {
# Build the file path based on the app_id
rds_file_control <- paste0("D:/Thesis/Files/appids_control", "/", app_id_treat, "_", update_code, ".rds")
rds_file_treat <- paste0("D:/Thesis/Files/appids_treatment", "/", app_id_treat, ".rds")
# Load data and filter immediately
appid_control <- readRDS(rds_file_control) %>% filter(steamid == steam_id, appid == app_id)
appid_trt <- readRDS(rds_file_treat) %>% filter(steamid == steam_id, appid == app_id, update_id == update_code)
# Check if the dataframes have zero rows based on treat_flg, and return 0 if true
if (treat_flg == 1 && nrow(appid_trt) == 0) {
return(0)
} else if (treat_flg == 0 && nrow(appid_control) == 0) {
return(0)
}
all_dates <- as.Date(colnames(appid_control)[3:length(colnames(appid_control))], format = "%Y-%m-%d")
date_indices_in_range <- which(all_dates >= date_ini & all_dates <= date_end)
# Avoid operations on whole dataframe if not necessary
if (length(date_indices_in_range) == 0) {
return(0)
}
if (treat_flg == 1) {
playtime_rows <- appid_trt
} else {
playtime_rows <- appid_control
}
# Add a warning if the number of rows in playtime_rows is not equal to 1
if (nrow(playtime_rows) != 1) {
warning("playtime_rows has more than one row. Please check the input data.")
}
if (treat_flg == 1) {
return(sum(playtime_rows[1, (seq_along(date_indices_in_range) + 3)], na.rm = TRUE))
} else {
return(sum(playtime_rows[1, (seq_along(date_indices_in_range) + 2)], na.rm = TRUE))
}
print(paste0("Processing file ", app_id_treat,"-", treat_flg, "."))
}
process_rds_file <- function(file, new_dir) {
library(foreach)
library(dplyr)
# Read the data from the RDS file
data_list <- readRDS(file)
# Initialize an empty list to store the combined results
combined_results <- list()
# Use foreach to parallelize the loop
foreach(j = seq_along(data_list), .combine = 'c', .packages = "dplyr") %dopar% {
data <- data_list[[j]]
total_playtime <- foreach(k = 1:nrow(data), .combine = c, .packages = "dplyr") %dopar% {
calculate_playtime_appid(
steam_id = data$steamid[k],
app_id = data$appid[k],
app_id_treat = data$appid_trt[k],
treat_flg = data$treat_flg[k],
update_code = data$update_id_trt[k],
date_ini = data$Date_Ini[k],
date_end = data$Date_End[k]
)
}
data <- data %>%
mutate(total_playtime = total_playtime)
# Save each dataframe to the list
combined_results[[length(combined_results) + 1]] <- data
}
# Save the modified list to a new folder with the new file name
new_file_name <- paste0("groups_playtime_", combined_results[[1]]$appid_trt[1], "_", combined_results[[1]]$update_id_trt[1], ".rds")
full_path <- file.path(pt_dir, new_file_name)
saveRDS(combined_results, file = full_path)
# Return the file path
return(full_path)
}
library(parallel)
library(doParallel)
# Set the directory where the RDS files are stored
new_dir <- "D:/Thesis/Files/ExpGroups/"
pt_dir <- "D:/Thesis/Files/ExpGroups_playtime/"
# List all the RDS files in the directory
rds_files <- list.files(path = new_dir, pattern = "*.rds", full.names = TRUE)
# Set up a cluster with multiple workers
cl <- makeCluster(detectCores())
# Register the cluster as the parallel backend for foreach
registerDoParallel(cl)
# Apply the function to each RDS file in parallel
file_paths <- foreach(file = rds_files, .combine = c) %dopar% {
process_rds_file(file, new_dir)
}
calculate_playtime_appid <- function(steam_id, app_id, app_id_treat, treat_flg, update_code, date_ini, date_end) {
# Build the file path based on the app_id
rds_file_control <- paste0("D:/Thesis/Files/appids_control", "/", app_id_treat, "_", update_code, ".rds")
rds_file_treat <- paste0("D:/Thesis/Files/appids_treatment", "/", app_id_treat, ".rds")
# Load data and filter immediately
appid_control <- distinct(readRDS(rds_file_control) %>% filter(steamid == steam_id, appid == app_id))
appid_trt <- distinct(readRDS(rds_file_treat) %>% filter(steamid == steam_id, appid == app_id, update_id == update_code))
# Check if the dataframes have zero rows based on treat_flg, and return 0 if true
if (treat_flg == 1 && nrow(appid_trt) == 0) {
return(0)
} else if (treat_flg == 0 && nrow(appid_control) == 0) {
return(0)
}
all_dates <- as.Date(colnames(appid_control)[3:length(colnames(appid_control))], format = "%Y-%m-%d")
date_indices_in_range <- which(all_dates >= date_ini & all_dates <= date_end)
# Avoid operations on whole dataframe if not necessary
if (length(date_indices_in_range) == 0) {
return(0)
}
if (treat_flg == 1) {
playtime_rows <- appid_trt
} else {
playtime_rows <- appid_control
}
# Add a warning if the number of rows in playtime_rows is not equal to 1
if (nrow(playtime_rows) != 1) {
warning("playtime_rows has more than one row. Please check the input data.")
}
if (treat_flg == 1) {
return(sum(playtime_rows[1, (seq_along(date_indices_in_range) + 3)], na.rm = TRUE))
} else {
return(sum(playtime_rows[1, (seq_along(date_indices_in_range) + 2)], na.rm = TRUE))
}
print(paste0("Processing file ", app_id_treat,"-", treat_flg, "."))
}
process_rds_file <- function(file, new_dir) {
library(foreach)
library(dplyr)
# Read the data from the RDS file
data_list <- readRDS(file)
# Initialize an empty list to store the combined results
combined_results <- list()
# Use foreach to parallelize the loop
foreach(j = seq_along(data_list), .combine = 'c', .packages = "dplyr") %dopar% {
data <- data_list[[j]]
total_playtime <- foreach(k = 1:nrow(data), .combine = c, .packages = "dplyr") %dopar% {
calculate_playtime_appid(
steam_id = data$steamid[k],
app_id = data$appid[k],
app_id_treat = data$appid_trt[k],
treat_flg = data$treat_flg[k],
update_code = data$update_id_trt[k],
date_ini = data$Date_Ini[k],
date_end = data$Date_End[k]
)
}
data <- data %>%
mutate(total_playtime = total_playtime)
# Save each dataframe to the list
combined_results[[length(combined_results) + 1]] <- data
}
# Save the modified list to a new folder with the new file name
new_file_name <- paste0("groups_playtime_", combined_results[[1]]$appid_trt[1], "_", combined_results[[1]]$update_id_trt[1], ".rds")
full_path <- file.path(pt_dir, new_file_name)
saveRDS(combined_results, file = full_path)
# Return the file path
return(full_path)
}
library(parallel)
library(doParallel)
# Set the directory where the RDS files are stored
new_dir <- "D:/Thesis/Files/ExpGroups/"
pt_dir <- "D:/Thesis/Files/ExpGroups_playtime/"
# List all the RDS files in the directory
rds_files <- list.files(path = new_dir, pattern = "*.rds", full.names = TRUE)
# Set up a cluster with multiple workers
cl <- makeCluster(detectCores())
# Register the cluster as the parallel backend for foreach
registerDoParallel(cl)
# Apply the function to each RDS file in parallel
file_paths <- foreach(file = rds_files, .combine = c) %dopar% {
process_rds_file(file, new_dir)
}
calculate_playtime_appid <- function(steam_id, app_id, app_id_treat, treat_flg, update_code, date_ini, date_end) {
# Build the file path based on the app_id
rds_file_control <- paste0("D:/Thesis/Files/appids_control", "/", app_id_treat, "_", update_code, ".rds")
rds_file_treat <- paste0("D:/Thesis/Files/appids_treatment", "/", app_id_treat, ".rds")
print(paste0("Path Loaded"))
# Load data and filter immediately
appid_control <- distinct(readRDS(rds_file_control) %>% filter(steamid == steam_id, appid == app_id))
appid_trt <- distinct(readRDS(rds_file_treat) %>% filter(steamid == steam_id, appid == app_id, update_id == update_code))
print(paste0("App Control and Treat Loaded"))
# Check if the data frames have zero rows based on treat_flg, and return 0 if true
if (treat_flg == 1 && nrow(appid_trt) == 0) {
return(0)
} else if (treat_flg == 0 && nrow(appid_control) == 0) {
return(0)
}
print(paste0("Zero Rows Checked"))
all_dates <- as.Date(colnames(appid_control)[3:length(colnames(appid_control))], format = "%Y-%m-%d")
date_indices_in_range <- which(all_dates >= date_ini & all_dates <= date_end)
print(paste0("Rows with no zeros on the dates checked"))
# Avoid operations on whole dataframe if not necessary
if (length(date_indices_in_range) == 0) {
return(0)
}
print(paste0("If no rows on date 2"))
if (treat_flg == 1) {
playtime_rows <- appid_trt
} else {
playtime_rows <- appid_control
}
print(paste0("play time rows loaded"))
# Add a warning if the number of rows in playtime_rows is not equal to 1
if (nrow(playtime_rows) != 1) {
warning("playtime_rows has more than one row. Please check the input data.")
}
if (treat_flg == 1) {
return(sum(playtime_rows[1, (seq_along(date_indices_in_range) + 3)], na.rm = TRUE))
} else {
return(sum(playtime_rows[1, (seq_along(date_indices_in_range) + 2)], na.rm = TRUE))
}
print(paste0("Processing file ", app_id_treat,"-", treat_flg, "."))
}
process_rds_file <- function(file, new_dir) {
library(foreach)
library(dplyr)
# Read the data from the RDS file
data_list <- readRDS(file)
# Initialize an empty list to store the combined results
combined_results <- list()
# Use foreach to parallelize the loop
foreach(j = seq_along(data_list), .combine = 'c', .packages = "dplyr") %dopar% {
data <- data_list[[j]]
total_playtime <- foreach(k = 1:nrow(data), .combine = c, .packages = "dplyr") %dopar% {
calculate_playtime_appid(
steam_id = data$steamid[k],
app_id = data$appid[k],
app_id_treat = data$appid_trt[k],
treat_flg = data$treat_flg[k],
update_code = data$update_id_trt[k],
date_ini = data$Date_Ini[k],
date_end = data$Date_End[k]
)
}
data <- data %>%
mutate(total_playtime = total_playtime)
# Save each dataframe to the list
combined_results[[length(combined_results) + 1]] <- data
}
# Save the modified list to a new folder with the new file name
new_file_name <- paste0("groups_playtime_", combined_results[[1]]$appid_trt[1], "_", combined_results[[1]]$update_id_trt[1], ".rds")
full_path <- file.path(pt_dir, new_file_name)
saveRDS(combined_results, file = full_path)
# Return the file path
return(full_path)
}
library(parallel)
library(doParallel)
# Set the directory where the RDS files are stored
new_dir <- "D:/Thesis/Files/ExpGroups/"
pt_dir <- "D:/Thesis/Files/ExpGroups_playtime/"
# List all the RDS files in the directory
rds_files <- list.files(path = new_dir, pattern = "*.rds", full.names = TRUE)
# Set up a cluster with multiple workers
cl <- makeCluster(detectCores())
# Register the cluster as the parallel backend for foreach
registerDoParallel(cl)
# Apply the function to each RDS file in parallel
file_paths <- foreach(file = rds_files, .combine = c) %dopar% {
process_rds_file(file, new_dir)
}
## --- LOADIN THE RDS ----- ##
zzcheck <- readRDS("D:/Thesis/Files/ExpGroups/groups_8500_8.rds")
View(zzcheck)
View(zzcheck)
# Stop the cluster
stopCluster(cl)
## --- LOADIN THE RDS ----- ##
zzcheck <- readRDS("D:/Thesis/Files/ExpGroups/groups_223750_28.rds")
View(zzcheck)
# Load required libraries
library(parallel)
library(doParallel)
library(foreach)
# Get the number of cores
num_cores <- detectCores()
# Set up a parallel backend to use all available cores
cl <- makeCluster(num_cores)
registerDoParallel(cl)
# Set new directory
new_dir <- "D:/Thesis/Files/ExpGroups/"
# Parallel foreach loop
results <- foreach(i = 1:nrow(unique_combinations), .combine = 'rbind', .packages = c("dplyr")) %dopar% {
app_id <- unique_combinations[i, 'appid']
update_id <- unique_combinations[i, 'marked_id_update']
control_ids <- exp_groups[exp_groups$appid == app_id & exp_groups$marked_id_update == update_id, 'appid_control']
# Initialize an empty data frame to store the combined results for the current combination
combined_result <- data.frame()
for (control_id in control_ids) {
result <- subset_experiment(final_steamid_upd_appid, app_id, update_id, control_id)
if (nrow(result) > 0) {
# Combine the results for the current combination
combined_result <- rbind(combined_result, result)
}
}
combined_result <- distinct(combined_result)
# Add appid and update_id columns of the treatment update of the experiment.
combined_result$appid_trt <- app_id
combined_result$update_id_trt <- update_id
# Left join information from final_user_summary by steamid
combined_result <- left_join(combined_result, final_user_summary, by = "steamid")
# Left join additional_data by appid and update_id
combined_result <- left_join(combined_result, updates_detail, by = c("appid_trt" = "appid", "update_id_trt" = "id_update"))
# Save the modified data frame to a new folder with the new file name
new_file_name <- paste0("groups_", app_id, "_", update_id, ".rds")
full_path <- paste0(new_dir, new_file_name)
saveRDS(combined_result, file = full_path)
combined_result
}
warnings()
# Stop the cluster
stopCluster(cl)
gc()
save.image("D:/Thesis/Files/Processed/Data Thesis.RData")
load("D:/Thesis/Files/Processed/Data Thesis.RData")
load("D:/Thesis/Files/Processed/Data Thesis.RData")
facilities_around_coordinates_parking <- read.csv("C:/Users/radok/OneDrive/Desktop/Maastricht Univeristy/Service Project/Business Analytics/API Extract/Open Street Map/facilities_around_coordinates_parking.csv")
View(facilities_around_coordinates_parking)
num_blank <- sum(facilities_around_coordinates_parking$postcode == "")
df_filtered <- facilities_around_coordinates_parking %>%
filter(facilities_around_coordinates_parking$postcode != "")
library(dplyr)
num_blank <- sum(facilities_around_coordinates_parking$postcode == "")
df_filtered <- facilities_around_coordinates_parking %>%
filter(facilities_around_coordinates_parking$postcode != "")
df_blank <- facilities_around_coordinates_parking %>%
filter(facilities_around_coordinates_parking$postcode = "")
df_blank <- facilities_around_coordinates_parking %>%
filter(facilities_around_coordinates_parking$postcode == "")
View(df_blank)
check <- df_blank %>%
group_by(postcode) %>%
mutate(count = n())
View(check)
write.csv(check,"check.csv")
facilities_around_coordinates <- read.csv("C:/Users/radok/OneDrive/Desktop/Maastricht Univeristy/Service Project/Business Analytics/API Extract/Open Street Map/facilities_around_coordinates.csv")
View(facilities_around_coordinates)
df_filtered <- facilities_around_coordinates %>%
filter(facilities_around_coordinates$type != "")
View(df_filtered)
source("C:/Users/radok/OneDrive/Desktop/Maastricht Univeristy/Service Project/Business Analytics/ZIP File for BA/Step_3/1 - Model Creation - Performance and Prediction.R", echo=TRUE)
